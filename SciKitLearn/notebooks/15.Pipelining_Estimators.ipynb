{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we study how different estimators maybe be chained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example: feature extraction and selection before an estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction: vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some types of data, for instance text data, a feature extraction step must be applied to convert it to numerical features.\n",
    "To illustrate we load the SMS spam dataset we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(os.path.join(\"datasets\", \"smsspam\", \"SMSSpamCollection\")) as f:\n",
    "    lines = [line.strip().split(\"\\t\") for line in f.readlines()]\n",
    "text = [x[1] for x in lines]\n",
    "y = [x[0] == \"ham\" for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train, text_test, y_train, y_test = train_test_split(text, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we applied the feature extraction manually, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afunn\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.975609756097561"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text_train)\n",
    "\n",
    "X_train = vectorizer.transform(text_train)\n",
    "X_test = vectorizer.transform(text_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The situation where we learn a transformation and then apply it to the test data is very common in machine learning.\n",
    "Therefore scikit-learn has a shortcut for this, called pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975609756097561"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "pipeline.fit(text_train, y_train)\n",
    "pipeline.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this makes the code much shorter and easier to handle. Behind the scenes, exactly the same as above is happening. When calling fit on the pipeline, it will call fit on each step in turn.\n",
    "\n",
    "After the first step is fit, it will use the ``transform`` method of the first step to create a new representation.\n",
    "This will then be fed to the ``fit`` of the next step, and so on.\n",
    "Finally, on the last step, only ``fit`` is called.\n",
    "\n",
    "![pipeline](figures/pipeline.svg)\n",
    "\n",
    "If we call ``score``, only ``transform`` will be called on each step - this could be the test set after all! Then, on the last step, ``score`` is called with the new representation. The same goes for ``predict``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building pipelines not only simplifies the code, it is also important for model selection.\n",
    "Say we want to grid-search C to tune our Logistic Regression above.\n",
    "\n",
    "Let's say we do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This illustrates a common mistake. Don't use this code!\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text_train)\n",
    "\n",
    "X_train = vectorizer.transform(text_train)\n",
    "X_test = vectorizer.transform(text_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "grid = GridSearchCV(clf, param_grid={'C': [.1, 1, 10, 100]}, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 What did we do wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we did grid-search with cross-validation on ``X_train``. However, when applying ``TfidfVectorizer``, it saw all of the ``X_train``,\n",
    "not only the training folds! So it could use knowledge of the frequency of the words in the test-folds. This is called \"contamination\" of the test set, and leads to too optimistic estimates of generalization performance, or badly selected parameters.\n",
    "We can fix this with the pipeline, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afunn\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9892395982783357"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(), \n",
    "                         LogisticRegression())\n",
    "\n",
    "grid = GridSearchCV(pipeline,\n",
    "                    param_grid={'logisticregression__C': [.1, 1, 10, 100]}, cv=5)\n",
    "\n",
    "grid.fit(text_train, y_train)\n",
    "grid.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to tell the pipeline where at which step we wanted to set the parameter ``C``.\n",
    "We can do this using the special ``__`` syntax. The name before the ``__`` is simply the name of the class, the part after ``__`` is the parameter we want to set with grid-search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/pipeline_cross_validation.svg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another benefit of using pipelines is that we can now also search over parameters of the feature extraction with ``GridSearchCV``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afunn\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 100, 'tfidfvectorizer__ngram_range': (1, 2)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9870875179340028"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "\n",
    "params = {'logisticregression__C': [.1, 1, 10, 100],\n",
    "          \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2)]}\n",
    "grid = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(grid.best_params_)\n",
    "grid.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Create a pipeline out of a StandardScaler and Ridge regression and apply it to the Boston housing dataset (load using ``sklearn.datasets.load_boston``). Try adding the ``sklearn.preprocessing.PolynomialFeatures`` transformer as a second preprocessing step, and grid-search the degree of the polynomials (try 1, 2 and 3).\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMC code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Create a pipeline out of a StandardScaler and Ridge regression and apply it to the Boston housing dataset (load using ``sklearn.datasets.load_boston``). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline #import the pipeline\n",
    "from sklearn.model_selection import GridSearchCV #import the cross validation search\n",
    "from sklearn.preprocessing import StandardScaler # import the standard scaler to normalise the data\n",
    "from sklearn.linear_model import Ridge #import the ridge regression estimator\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n",
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9]\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "#let's take a look at the dataset\n",
    "print(boston.data.shape)\n",
    "print(boston.target.shape)\n",
    "print(boston.target[0:10])\n",
    "print(boston.feature_names)\n",
    "#print(boston.DESCR)\n",
    "\n",
    "X = boston.data #define the data\n",
    "y = boston.target #define the labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state =42) #split the data into train / test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the score of this fit ridge regression model is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.684167032652479"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = make_pipeline(StandardScaler(), #define the pipeline\n",
    "                         Ridge())\n",
    "\n",
    "pipeline.fit(X_train, y_train) #fit the pipeline to the data and to the labels\n",
    "\n",
    "print(\"the score of this fit ridge regression model is: \")\n",
    "pipeline.score(X_test, y_test) #score the fit ridge regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Try adding the ``sklearn.preprocessing.PolynomialFeatures`` transformer as a second preprocessing step, and grid-search the degree of the polynomials (try 1, 2 and 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV] polynomialfeatures__degree=1 ....................................\n",
      "[CV]  polynomialfeatures__degree=1, score=0.7353297651715035, total=   0.0s\n",
      "[CV] polynomialfeatures__degree=1 ....................................\n",
      "[CV]  polynomialfeatures__degree=1, score=0.64193627457653, total=   0.0s\n",
      "[CV] polynomialfeatures__degree=1 ....................................\n",
      "[CV]  polynomialfeatures__degree=1, score=0.7633074667258515, total=   0.0s\n",
      "[CV] polynomialfeatures__degree=1 ....................................\n",
      "[CV]  polynomialfeatures__degree=1, score=0.7755928134579677, total=   0.0s\n",
      "[CV] polynomialfeatures__degree=1 ...................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  polynomialfeatures__degree=1, score=0.6624937843076837, total=   0.0s\n",
      "[CV] polynomialfeatures__degree=2 ....................................\n",
      "[CV]  polynomialfeatures__degree=2, score=0.7463242295088154, total=   0.0s\n",
      "[CV] polynomialfeatures__degree=2 ....................................\n",
      "[CV]  polynomialfeatures__degree=2, score=0.8248323349449325, total=   0.0s\n",
      "[CV] polynomialfeatures__degree=2 ....................................\n",
      "[CV]  polynomialfeatures__degree=2, score=0.841466082718046, total=   0.0s\n",
      "[CV] polynomialfeatures__degree=2 ....................................\n",
      "[CV]  polynomialfeatures__degree=2, score=0.9140569325391539, total=   0.0s\n",
      "[CV] polynomialfeatures__degree=2 ....................................\n",
      "[CV]  polynomialfeatures__degree=2, score=0.7945341457872482, total=   0.0s\n",
      "[CV] polynomialfeatures__degree=3 ....................................\n",
      "[CV]  polynomialfeatures__degree=3, score=0.42757152143216703, total=   0.4s\n",
      "[CV] polynomialfeatures__degree=3 ....................................\n",
      "[CV]  polynomialfeatures__degree=3, score=0.7829559410963854, total=   0.1s\n",
      "[CV] polynomialfeatures__degree=3 ....................................\n",
      "[CV]  polynomialfeatures__degree=3, score=-4.772554150144874, total=   0.1s\n",
      "[CV] polynomialfeatures__degree=3 ....................................\n",
      "[CV]  polynomialfeatures__degree=3, score=0.8382884157635928, total=   0.1s\n",
      "[CV] polynomialfeatures__degree=3 ....................................\n",
      "[CV]  polynomialfeatures__degree=3, score=0.7167595641269401, total=   0.1s\n",
      "{'polynomialfeatures__degree': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    2.2s finished\n",
      "C:\\Users\\afunn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7969314466714326"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures #import the polynomial features transformer\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), \n",
    "                         PolynomialFeatures(), \n",
    "                         Ridge()) #re-define the pipeline\n",
    "\n",
    "grid = GridSearchCV(pipeline,\n",
    "                    param_grid={'polynomialfeatures__degree': [1,2,3]}, cv=5, verbose=3) #define a grid search\n",
    "\n",
    "\n",
    "\n",
    "grid.fit(X_train, y_train) #fit the grid\n",
    "\n",
    "print(grid.best_params_)\n",
    "grid.score(X_test, y_test) #use the highest scoring model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PolynomialFeatures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'polynomialfeatures__degree': 2}\n",
      "best score: 0.8168568802558642\n",
      "test score: 0.8316280118312019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afunn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/15A_ridge_grid.py\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "boston = load_boston()\n",
    "text_train, text_test, y_train, y_test = train_test_split(boston.data,\n",
    "                                                          boston.target,\n",
    "                                                          test_size=0.25,\n",
    "                                                          random_state=123)\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(),\n",
    "                         PolynomialFeatures(),\n",
    "                         Ridge())\n",
    "\n",
    "grid = GridSearchCV(pipeline,\n",
    "                    param_grid={'polynomialfeatures__degree': [1, 2, 3]}, cv=5)\n",
    "\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print('best parameters:', grid.best_params_)\n",
    "print('best score:', grid.best_score_)\n",
    "print('test score:', grid.score(text_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

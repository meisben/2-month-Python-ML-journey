{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods - Text Feature Extraction with Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many tasks, like in the classical spam detection, your input data is text.\n",
    "Free text with variables length is very far from the fixed length numeric representation that we need to do machine learning with scikit-learn.\n",
    "However, there is an easy and effective way to go from text data to a numeric representation using the so-called bag-of-words model, which provides a data structure that is compatible with the machine learning aglorithms in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/bag_of_words.svg\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that each sample in your dataset is represented as one string, which could be just a sentence, an email, or a whole news article or book. To represent the sample, we first split the string into a list of tokens, which correspond to (somewhat normalized) words. A simple way to do this to just split by whitespace, and then lowercase the word. \n",
    "\n",
    "Then, we build a vocabulary of all tokens (lowercased words) that appear in our whole dataset. This is usually a very large vocabulary.\n",
    "Finally, looking at our single sample, we could show how often each word in the vocabulary appears.\n",
    "We represent our string by a vector, where each entry is how often a given word in the vocabulary appears in the string.\n",
    "\n",
    "As each sample will only contain very few words, most entries will be zero, leading to a very high-dimensional but sparse representation.\n",
    "\n",
    "The method is called \"bag-of-words,\" as the order of the words is lost entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"Some say the world will end in fire,\",\n",
    "     \"Some say in ice.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'some': 5,\n",
       " 'say': 4,\n",
       " 'the': 6,\n",
       " 'world': 8,\n",
       " 'will': 7,\n",
       " 'end': 0,\n",
       " 'in': 3,\n",
       " 'fire': 1,\n",
       " 'ice': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bag_of_words = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end', 'fire', 'ice', 'in', 'say', 'some', 'the', 'will', 'world']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['end', 'fire', 'in', 'say', 'some', 'the', 'will', 'world'],\n",
       "       dtype='<U5'), array(['ice', 'in', 'say', 'some'], dtype='<U5')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(X_bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf Encoding\n",
    "A useful transformation that is often applied to the bag-of-word encoding is the so-called term-frequency inverse-document-frequency (tf-idf) scaling, which is a non-linear transformation of the word counts.\n",
    "\n",
    "The tf-idf encoding rescales words that are common to have less weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39 0.39 0.   0.28 0.28 0.28 0.39 0.39 0.39]\n",
      " [0.   0.   0.63 0.45 0.45 0.45 0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(tfidf_vectorizer.transform(X).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idfs are a way to represent documents as feature vectors. tf-idfs can be understood as a modification of the raw term frequencies (`tf`); the `tf` is the count of how often a particular word occurs in a given document. The concept behind the tf-idf is to downweight terms proportionally to the number of documents in which they occur. Here, the idea is that terms that occur in many different documents are likely unimportant or don't contain any useful information for Natural Language Processing tasks such as document classification. If you are interested in the mathematical details and equations, see this [external IPython Notebook](http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/tfidf_scikit-learn.ipynb) that walks you through the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams and N-Grams\n",
    "\n",
    "In the example illustrated in the figure at the beginning of this notebook, we used the so-called 1-gram (unigram) tokenization: Each token represents a single element with regard to the splittling criterion. \n",
    "\n",
    "Entirely discarding word order is not always a good idea, as composite phrases often have specific meaning, and modifiers like \"not\" can invert the meaning of words.\n",
    "\n",
    "A simple way to include some word order are n-grams, which don't only look at a single token, but at all pairs of neighborhing tokens. For example, in 2-gram (bigram) tokenization, we would group words together with an overlap of one word; in 3-gram (trigram) splits we would create an overlap two words, and so forth:\n",
    "\n",
    "- original text: \"this is how you get ants\"\n",
    "- 1-gram: \"this\", \"is\", \"how\", \"you\", \"get\", \"ants\"\n",
    "- 2-gram: \"this is\", \"is how\", \"how you\", \"you get\", \"get ants\"\n",
    "- 3-gram: \"this is how\", \"is how you\", \"how you get\", \"you get ants\"\n",
    "\n",
    "Which \"n\" we choose for \"n-gram\" tokenization to obtain the optimal performance in our predictive model depends on the learning algorithm, dataset, and task. Or in other words, we have consider \"n\" in \"n-grams\" as a tuning parameters, and in later notebooks, we will see how we deal with these.\n",
    "\n",
    "Now, let's create a bag of words model of bigrams using scikit-learn's `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at sequences of tokens of minimum length 2 and maximum length 2\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "bigram_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end in',\n",
       " 'in fire',\n",
       " 'in ice',\n",
       " 'say in',\n",
       " 'say the',\n",
       " 'some say',\n",
       " 'the world',\n",
       " 'will end',\n",
       " 'world will']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to include unigrams (single tokens) AND bigrams, wich we can do by passing the following tuple as an argument to the `ngram_range` parameter of the `CountVectorizer` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "gram_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end',\n",
       " 'end in',\n",
       " 'fire',\n",
       " 'ice',\n",
       " 'in',\n",
       " 'in fire',\n",
       " 'in ice',\n",
       " 'say',\n",
       " 'say in',\n",
       " 'say the',\n",
       " 'some',\n",
       " 'some say',\n",
       " 'the',\n",
       " 'the world',\n",
       " 'will',\n",
       " 'will end',\n",
       " 'world',\n",
       " 'world will']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character n-grams\n",
    "=================\n",
    "\n",
    "Sometimes it is also helpful not only to look at words, but to consider single characters instead.   \n",
    "That is particularly useful if we have very noisy data and want to identify the language, or if we want to predict something about a single word.\n",
    "We can simply look at characters instead of words by setting ``analyzer=\"char\"``.\n",
    "Looking at single characters is usually not very informative, but looking at longer n-grams of characters could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some say the world will end in fire,', 'Some say in ice.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vectorizer = CountVectorizer(ngram_range=(2, 2), analyzer=\"char\")\n",
    "char_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' e', ' f', ' i', ' s', ' t', ' w', 'ay', 'ce', 'd ', 'e ', 'e,', 'e.', 'en', 'fi', 'he', 'ic', 'il', 'in', 'ir', 'l ', 'ld', 'll', 'me', 'n ', 'nd', 'om', 'or', 're', 'rl', 'sa', 'so', 'th', 'wi', 'wo', 'y ']\n"
     ]
    }
   ],
   "source": [
    "print(char_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Compute the bigrams from \"zen of python\" as given below (or by ``import this``), and find the most common trigram.\n",
    "We want to treat each line as a separate document. You can achieve this by splitting the string by newlines (``\\n``).\n",
    "Compute the Tf-idf encoding of the data. Which words have the highest tf-idf score? Why?\n",
    "What changes if you use ``TfidfVectorizer(norm=\"none\")``?\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "zen = \"\"\"Beautiful is better than ugly.\n",
    "Explicit is better than implicit.\n",
    "Simple is better than complex.\n",
    "Complex is better than complicated.\n",
    "Flat is better than nested.\n",
    "Sparse is better than dense.\n",
    "Readability counts.\n",
    "Special cases aren't special enough to break the rules.\n",
    "Although practicality beats purity.\n",
    "Errors should never pass silently.\n",
    "Unless explicitly silenced.\n",
    "In the face of ambiguity, refuse the temptation to guess.\n",
    "There should be one-- and preferably only one --obvious way to do it.\n",
    "Although that way may not be obvious at first unless you're Dutch.\n",
    "Now is better than never.\n",
    "Although never is often better than *right* now.\n",
    "If the implementation is hard to explain, it's a bad idea.\n",
    "If the implementation is easy to explain, it may be a good idea.\n",
    "Namespaces are one honking great idea -- let's do more of those!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMC solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Sparse is better than dense.\n"
     ]
    }
   ],
   "source": [
    "text = zen.split(\"\\n\")\n",
    "print(len(text))\n",
    "print(text[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #import estimator\n",
    "\n",
    "\n",
    "# look at sequences of tokens of minimum length 2 and maximum length 2\n",
    "bigram_vectorizer2 = CountVectorizer(ngram_range=(2, 2)) #instantiate\n",
    "bigram_vectorizer2.fit(text) #fit estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['although never',\n",
       " 'although practicality',\n",
       " 'although that',\n",
       " 'ambiguity refuse',\n",
       " 'and preferably',\n",
       " 'are one',\n",
       " 'aren special',\n",
       " 'at first',\n",
       " 'bad idea',\n",
       " 'be good',\n",
       " 'be obvious',\n",
       " 'be one',\n",
       " 'beats purity',\n",
       " 'beautiful is',\n",
       " 'better than',\n",
       " 'break the',\n",
       " 'cases aren',\n",
       " 'complex is',\n",
       " 'do it',\n",
       " 'do more',\n",
       " 'easy to',\n",
       " 'enough to',\n",
       " 'errors should',\n",
       " 'explain it',\n",
       " 'explicit is',\n",
       " 'explicitly silenced',\n",
       " 'face of',\n",
       " 'first unless',\n",
       " 'flat is',\n",
       " 'good idea',\n",
       " 'great idea',\n",
       " 'hard to',\n",
       " 'honking great',\n",
       " 'idea let',\n",
       " 'if the',\n",
       " 'implementation is',\n",
       " 'in the',\n",
       " 'is better',\n",
       " 'is easy',\n",
       " 'is hard',\n",
       " 'is often',\n",
       " 'it bad',\n",
       " 'it may',\n",
       " 'let do',\n",
       " 'may be',\n",
       " 'may not',\n",
       " 'more of',\n",
       " 'namespaces are',\n",
       " 'never is',\n",
       " 'never pass',\n",
       " 'not be',\n",
       " 'now is',\n",
       " 'obvious at',\n",
       " 'obvious way',\n",
       " 'of ambiguity',\n",
       " 'of those',\n",
       " 'often better',\n",
       " 'one and',\n",
       " 'one honking',\n",
       " 'one obvious',\n",
       " 'only one',\n",
       " 'pass silently',\n",
       " 'practicality beats',\n",
       " 'preferably only',\n",
       " 're dutch',\n",
       " 'readability counts',\n",
       " 'refuse the',\n",
       " 'right now',\n",
       " 'should be',\n",
       " 'should never',\n",
       " 'simple is',\n",
       " 'sparse is',\n",
       " 'special cases',\n",
       " 'special enough',\n",
       " 'temptation to',\n",
       " 'than complex',\n",
       " 'than complicated',\n",
       " 'than dense',\n",
       " 'than implicit',\n",
       " 'than nested',\n",
       " 'than never',\n",
       " 'than right',\n",
       " 'than ugly',\n",
       " 'that way',\n",
       " 'the face',\n",
       " 'the implementation',\n",
       " 'the rules',\n",
       " 'the temptation',\n",
       " 'there should',\n",
       " 'to break',\n",
       " 'to do',\n",
       " 'to explain',\n",
       " 'to guess',\n",
       " 'unless explicitly',\n",
       " 'unless you',\n",
       " 'way may',\n",
       " 'way to',\n",
       " 'you re']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureNames = bigram_vectorizer2.get_feature_names()\n",
    "bigram_vectorizer2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 98)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 8 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 2 1\n",
      " 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1]\n",
      "['better than']\n",
      "\n",
      "Solution 1: The most common bi-gram is ['better than']\n"
     ]
    }
   ],
   "source": [
    "print(bigram_vectorizer2.transform(text).toarray().shape)\n",
    "print(bigram_vectorizer2.transform(text).toarray().sum(axis=0)) #find maximum word counts\n",
    "\n",
    "maxes = bigram_vectorizer2.transform(text).toarray().sum(axis=0)\n",
    "\n",
    "max_pos = np.where(maxes == np.amax(maxes))\n",
    "\n",
    "featureNames = np.array(featureNames)\n",
    "\n",
    "print(featureNames[max_pos])\n",
    "\n",
    "print(\"\")\n",
    "print(\"Solution 1: The most common bi-gram is {}\".format(featureNames[max_pos]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 79)\n",
      "[0.42 0.33 0.29 0.32 0.32 0.3  0.38 0.28 0.53 0.59 0.37 0.32 0.32 0.54\n",
      " 0.62 0.71 0.59 0.28 0.3  0.35 0.32 0.48 0.34 0.59 0.6  0.33 0.3  0.59\n",
      " 0.35 0.32 0.33 0.38 0.32 0.3  0.34 0.34 0.59 0.33 0.32 0.3  0.32 0.31\n",
      " 0.32 0.32 0.59 0.53 0.3  0.59 0.26 0.29 0.45 0.51 0.29 0.48 0.53 0.29\n",
      " 0.53 0.3  0.71 0.33 0.45 0.32 0.42 0.6  0.48 0.62 0.59 0.63 0.33 0.37\n",
      " 0.3  0.47 0.29 0.32 0.26 0.59 0.53 0.26 0.3 ]\n",
      "\n",
      "(array([15, 58], dtype=int64),)\n",
      "['counts' 'readability']\n",
      "\n",
      "Solution 2a: The highest tf-idf score is for ['counts' 'readability']\n",
      "\n",
      "Solution 2b: The highest tf-idf scores are for ['complicated' 'counts' 'explicitly' 'readability' 'silenced' 'simple'\n",
      " 'special']\n"
     ]
    }
   ],
   "source": [
    "## Compute the tf-idf encoding\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#tfidf_vectorizer2 = TfidfVectorizer(norm=\"l2\")\n",
    "tfidf_vectorizer2.fit(text)\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(tfidf_vectorizer2.transform(text).toarray().shape)\n",
    "print(tfidf_vectorizer2.transform(text).toarray().max(axis=0)) #find maximum word counts\n",
    "\n",
    "maxes2 = tfidf_vectorizer2.transform(text).toarray().max(axis=0)\n",
    "maxes2 = tfidf_vectorizer2.transform(text).toarray().max(axis=0)\n",
    "\n",
    "max_pos2 = np.where(maxes2 == np.amax(maxes2))\n",
    "max_list = np.where(maxes2 > 0.6)\n",
    "\n",
    "print(\"\")\n",
    "print(max_pos2)\n",
    "\n",
    "featureNames2 = tfidf_vectorizer2.get_feature_names()\n",
    "\n",
    "featureNames2 = np.array(featureNames2)\n",
    "\n",
    "print(featureNames2[max_pos2])\n",
    "\n",
    "print(\"\")\n",
    "print(\"Solution 2a: The highest tf-idf score is for {}\".format(featureNames2[max_pos2]))\n",
    "print(\"\")\n",
    "print(\"Solution 2b: The highest tf-idf scores are for {}\".format(featureNames2[max_list]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['although',\n",
       " 'ambiguity',\n",
       " 'and',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'at',\n",
       " 'bad',\n",
       " 'be',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'better',\n",
       " 'break',\n",
       " 'cases',\n",
       " 'complex',\n",
       " 'complicated',\n",
       " 'counts',\n",
       " 'dense',\n",
       " 'do',\n",
       " 'dutch',\n",
       " 'easy',\n",
       " 'enough',\n",
       " 'errors',\n",
       " 'explain',\n",
       " 'explicit',\n",
       " 'explicitly',\n",
       " 'face',\n",
       " 'first',\n",
       " 'flat',\n",
       " 'good',\n",
       " 'great',\n",
       " 'guess',\n",
       " 'hard',\n",
       " 'honking',\n",
       " 'idea',\n",
       " 'if',\n",
       " 'implementation',\n",
       " 'implicit',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'let',\n",
       " 'may',\n",
       " 'more',\n",
       " 'namespaces',\n",
       " 'nested',\n",
       " 'never',\n",
       " 'not',\n",
       " 'now',\n",
       " 'obvious',\n",
       " 'of',\n",
       " 'often',\n",
       " 'one',\n",
       " 'only',\n",
       " 'pass',\n",
       " 'practicality',\n",
       " 'preferably',\n",
       " 'purity',\n",
       " 're',\n",
       " 'readability',\n",
       " 'refuse',\n",
       " 'right',\n",
       " 'rules',\n",
       " 'should',\n",
       " 'silenced',\n",
       " 'silently',\n",
       " 'simple',\n",
       " 'sparse',\n",
       " 'special',\n",
       " 'temptation',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'there',\n",
       " 'those',\n",
       " 'to',\n",
       " 'ugly',\n",
       " 'unless',\n",
       " 'way',\n",
       " 'you']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer2.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common 2-gram: better than\n",
      "most common 3-gram: is better than\n",
      "most common 4-gram: if the implementation is\n",
      "highest tf-idf with norm=l2: counts\n",
      "highest tf-idf with norm=None: special\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/11_ngrams.py\n",
    "text = zen.split(\"\\n\")\n",
    "for n in [2, 3, 4]:\n",
    "    cv = CountVectorizer(ngram_range=(n, n)).fit(text)\n",
    "    counts = cv.transform(text)\n",
    "    most_common = np.argmax(counts.sum(axis=0))\n",
    "    print(\"most common %d-gram: %s\" % (n, cv.get_feature_names()[most_common]))\n",
    "\n",
    "\n",
    "for norm in [\"l2\", None]:\n",
    "    tfidf_vect = TfidfVectorizer(norm=norm).fit(text)\n",
    "    data_tfidf = tfidf_vect.transform(text) \n",
    "    most_common = tfidf_vect.get_feature_names()[np.argmax(data_tfidf.max(axis=0).toarray())]\n",
    "    print(\"highest tf-idf with norm=%s: %s\" % (norm, most_common))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
